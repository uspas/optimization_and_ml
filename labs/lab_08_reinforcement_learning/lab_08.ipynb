{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zBh0MnlFK1g",
        "outputId": "c9ce2a74-65dc-4c31-e8f9-70c6511a0921"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nA8WhpwFadc"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd /content/drive/MyDrive/optimization_and_ml/labs/lab_08_reinforcement_learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0c3Eb_9FJ1w"
      },
      "source": [
        "# Lab 09 - Reinforcement Learning\n",
        "\n",
        "In reinforcement learning (RL), an \"agent\" chooses actions given information about the present state of the system, and is given a reward (a measure of how good the action was). The aim is to maximize the cumulative returned reward over many interactions with the system.\n",
        "\n",
        "The problem is defined as a series of state transitions consisting of: State --> Action --> New State + reward.\n",
        "\n",
        "Multiple sequences make up an \"episode\" consisting of N state transitions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkyX48xEFJ1x"
      },
      "source": [
        "In this example, we'll look at the Deep Deterministic Policy Gradient (DDPG) algorithm (see paper [here](https://arxiv.org/abs/1509.02971)), for the standard **pendulum problem** (balancing a pendulum upright by applying torque). See problem details here https://www.gymlibrary.dev/environments/classic_control/pendulum/.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DDPG is an example of actor-critic RL where a mapping from observed to actions (i.e. a  \"policy\" or \"actor\") and an estimate of the long-term value of taking actions in a given state (i.e. a \"critic\") are both learned functions parameterized by neural networks. The critic provides the training signal for the actor, and the returned rewards provide the training signal for the critic.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq0fRVUpFJ1x"
      },
      "source": [
        "# Set up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-29T22:12:29.922893Z",
          "start_time": "2024-12-29T22:12:29.615682Z"
        },
        "id": "djvlix_xFJ1y"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "sys.path.append(os.getcwd())\n",
        "sys.path.append('/content/drive/MyDrive/optimization_and_ml/labs/lab_08_reinforcement_learning')#replace path as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-29T22:12:34.847716800Z",
          "start_time": "2024-12-29T22:12:29.654496100Z"
        },
        "id": "beS7t8ViFJ1y"
      },
      "outputs": [],
      "source": [
        "%reset -f\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import pickle as pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "\n",
        "from os import path\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDQHrH0KFJ1z"
      },
      "source": [
        "# Take a look at the DDPGAgent class below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-29T22:12:34.912263300Z",
          "start_time": "2024-12-29T22:12:34.863352400Z"
        },
        "id": "Ax5KpGAJFJ1z"
      },
      "outputs": [],
      "source": [
        "from ddpg.models import Critic, Actor\n",
        "from common.exp_replay import ExperienceReplayLog\n",
        "from common.noise import OUNoise\n",
        "\n",
        "\n",
        "class DDPGAgent:\n",
        "\n",
        "    def __init__(self, env, gamma, tau, buffer_maxlen, critic_learning_rate, actor_learning_rate, max_action = 1):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.env = env\n",
        "        self.obs_dim = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "        self.noise = OUNoise(env.action_space)\n",
        "        self.iter = 0.0\n",
        "        self.noisy = False\n",
        "\n",
        "        self.max_action= max_action\n",
        "\n",
        "        #print(self.action_dim)\n",
        "        #print(self.obs_dim)\n",
        "\n",
        "        # RL options\n",
        "        self.env = env\n",
        "        self.gamma = gamma #discount factor\n",
        "        self.tau = tau #target network updates\n",
        "\n",
        "        # Initialize critic and actor networks\n",
        "        self.critic = Critic(self.obs_dim, self.action_dim).to(self.device)\n",
        "        self.critic_target = Critic(self.obs_dim, self.action_dim).to(self.device)\n",
        "\n",
        "        self.actor = Actor(self.obs_dim, self.action_dim,self.max_action).to(self.device)\n",
        "        self.actor_target = Actor(self.obs_dim, self.action_dim).to(self.device)\n",
        "\n",
        "        # Copy target network paramters for critic\n",
        "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "\n",
        "        # Set Optimization algorithms\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_learning_rate)\n",
        "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=actor_learning_rate)\n",
        "\n",
        "        self.replay_buffer = ExperienceReplayLog(buffer_maxlen)\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        #print('obs;',obs)\n",
        "\n",
        "        if self.noisy == True:\n",
        "            state = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
        "            action = self.actor.forward(state)\n",
        "            action = action.squeeze(0).cpu().detach().numpy()\n",
        "            action = self.noise.get_action(action,self.iter)\n",
        "            self.iter = self.iter+1\n",
        "\n",
        "        else:\n",
        "            state = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
        "            action = self.actor.forward(state)\n",
        "            action = action.squeeze(0).cpu().detach().numpy()\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update(self, batch_size):\n",
        "\n",
        "        #Batch updates\n",
        "        states, actions, rewards, next_states, _ = self.replay_buffer.sample(batch_size)\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, masks = self.replay_buffer.sample(batch_size)\n",
        "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
        "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
        "        reward_batch = torch.FloatTensor(reward_batch).to(self.device)\n",
        "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
        "        masks = torch.FloatTensor(masks).to(self.device)\n",
        "\n",
        "        # Q info updates\n",
        "        curr_Q = self.critic.forward(state_batch, action_batch)\n",
        "        next_actions = self.actor_target.forward(next_state_batch)\n",
        "        next_Q = self.critic_target.forward(next_state_batch, next_actions.detach())\n",
        "        expected_Q = reward_batch + self.gamma * next_Q\n",
        "\n",
        "        # Update Critic network\n",
        "        q_loss = F.mse_loss(curr_Q, expected_Q.detach())\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        q_loss.backward()\n",
        "\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Update Actor network\n",
        "        policy_loss = -self.critic.forward(state_batch, self.actor.forward(state_batch)).mean()\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Update Actor and Critic target networks\n",
        "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
        "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
        "\n",
        "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3PdA-30FJ1z"
      },
      "source": [
        "<a id='pendulum'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8imy3NDFJ1z"
      },
      "source": [
        "# Pendulum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bucQNc9FJ10"
      },
      "source": [
        "Take a look at how the Pendulum environment is defined in openAI gym below (this is the standard openAI gym implementation, and we've added additional helper functions for plotting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-29T22:12:34.927807400Z",
          "start_time": "2024-12-29T22:12:34.912263300Z"
        },
        "id": "d7FKahgtFJ10"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-29T22:12:35.015125900Z",
          "start_time": "2024-12-29T22:12:34.943466300Z"
        },
        "id": "lqKdp3qtFJ10"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PendulumEnv(gym.Env):\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array'],\n",
        "        'video.frames_per_second': 30\n",
        "    }\n",
        "\n",
        "    def __init__(self, g=10.0):\n",
        "        self.max_speed = 8\n",
        "        self.max_torque = 2.\n",
        "        self.dt = .05\n",
        "        self.g = g\n",
        "        self.m = 1.\n",
        "        self.l = 1.\n",
        "        self.viewer = None\n",
        "\n",
        "        #define action and observation space\n",
        "        high = np.array([1., 1., self.max_speed], dtype=np.float32)\n",
        "\n",
        "        self.action_space = spaces.Box(\n",
        "            low=-self.max_torque,\n",
        "            high=self.max_torque, shape=(1,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-high,\n",
        "            high=high,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.seed()\n",
        "\n",
        "        #lists for plotting\n",
        "        self.q1s=[]\n",
        "        self.costs=[]\n",
        "        self.reset_ep=[]\n",
        "        self.plot = True\n",
        "\n",
        "\n",
        "    def log_results(self,q11, reward, reset_ep_flag  = False):\n",
        "\n",
        "        '''\n",
        "        logs the results of a given iteration, for quad inputs, the reward, and whether the episode was resett\n",
        "\n",
        "        '''\n",
        "\n",
        "        self.costs.append(reward)\n",
        "        self.q1s.append(q11)\n",
        "\n",
        "\n",
        "\n",
        "        if reset_ep_flag == True:\n",
        "            self.reset_ep.append(1)\n",
        "\n",
        "        else:\n",
        "            self.reset_ep.append(0)\n",
        "\n",
        "    def plot_results(self,):\n",
        "\n",
        "        '''\n",
        "        plots the results from the logged values\n",
        "\n",
        "        '''\n",
        "\n",
        "        clear_output(wait=True)\n",
        "\n",
        "\n",
        "        f = plt.figure(figsize=(25,3))\n",
        "        ax = f.add_subplot(141)\n",
        "        ax2 = f.add_subplot(142)\n",
        "\n",
        "\n",
        "        plot_reset = np.where(np.array(self.reset_ep)==1)[0]\n",
        "        for i in range(0, len(plot_reset)):\n",
        "            ax.axvline(x=plot_reset[i],alpha=0.1,color='k')\n",
        "            ax2.axvline(x=plot_reset[i],alpha=0.1,color='k')\n",
        "\n",
        "        ax.plot(self.q1s,'.')\n",
        "        ax.set_ylabel('action',fontsize=12)\n",
        "        ax2.plot(self.costs, 'k.')\n",
        "        ax2.set_ylabel('reward',fontsize=12)\n",
        "\n",
        "        ax.set_xlabel('Iteration',fontsize=12)\n",
        "        ax2.set_xlabel('Iteration',fontsize=12)\n",
        "\n",
        "\n",
        "\n",
        "        plt.show();\n",
        "\n",
        "    def reset_plot(self,):\n",
        "\n",
        "        '''\n",
        "        resets the logs and the plot\n",
        "\n",
        "        '''\n",
        "        self.costs=[]\n",
        "        self.q1s=[]\n",
        "        self.reset_ep=[]\n",
        "\n",
        "\n",
        "    def save_plot(self,name = 'mon_'):\n",
        "\n",
        "        '''\n",
        "        saves results from the logged values to a json file\n",
        "\n",
        "        '''\n",
        "\n",
        "        run_details = {\n",
        "            'q1': self.q1s,\n",
        "            'costs': self.costs,\n",
        "            'reset_ep': self.reset_ep,\n",
        "        }\n",
        "\n",
        "        with open(name + '.json', 'w') as json_file:\n",
        "            json.dump(run_details, json_file, default = to_serializable)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, u):\n",
        "        th, thdot = self.state  # th := theta\n",
        "\n",
        "        g = self.g #gravity\n",
        "        m = self.m #mass\n",
        "        l = self.l #pendulum length\n",
        "        dt = self.dt #timestep\n",
        "\n",
        "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
        "        self.last_u = u  # for rendering\n",
        "        costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)\n",
        "\n",
        "        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt\n",
        "        newth = th + newthdot * dt\n",
        "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
        "\n",
        "\n",
        "\n",
        "        self.log_results(u, -costs, reset_ep_flag = False)\n",
        "\n",
        "\n",
        "        if self.plot == True:\n",
        "            self.plot_results()\n",
        "\n",
        "        else:\n",
        "            clear_output()\n",
        "\n",
        "        self.state = np.array([newth, newthdot])\n",
        "\n",
        "\n",
        "        return self._get_obs(), -costs, False, {}\n",
        "\n",
        "    def reset(self):\n",
        "        high = np.array([np.pi, 1])\n",
        "        self.state = self.np_random.uniform(low=-high, high=high)\n",
        "        self.last_u = None\n",
        "\n",
        "        self.log_results(np.nan, np.nan, reset_ep_flag = True)\n",
        "\n",
        "\n",
        "        if self.plot == True:\n",
        "            self.plot_results()\n",
        "\n",
        "        else:\n",
        "            clear_output()\n",
        "\n",
        "\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        theta, thetadot = self.state\n",
        "        return np.array([np.cos(theta), np.sin(theta), thetadot])\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(500, 500)\n",
        "            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)\n",
        "            rod = rendering.make_capsule(1, .2)\n",
        "            rod.set_color(.8, .3, .3)\n",
        "            self.pole_transform = rendering.Transform()\n",
        "            rod.add_attr(self.pole_transform)\n",
        "            self.viewer.add_geom(rod)\n",
        "            axle = rendering.make_circle(.05)\n",
        "            axle.set_color(0, 0, 0)\n",
        "            self.viewer.add_geom(axle)\n",
        "            fname = path.join(path.dirname(__file__), \"assets/clockwise.png\")\n",
        "            self.img = rendering.Image(fname, 1., 1.)\n",
        "            self.imgtrans = rendering.Transform()\n",
        "            self.img.add_attr(self.imgtrans)\n",
        "\n",
        "        self.viewer.add_onetime(self.img)\n",
        "        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)\n",
        "        if self.last_u:\n",
        "            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
        "\n",
        "    def close(self):\n",
        "        if self.viewer:\n",
        "            self.viewer.close()\n",
        "            self.viewer = None\n",
        "\n",
        "\n",
        "def angle_normalize(x):\n",
        "    return (((x+np.pi) % (2*np.pi)) - np.pi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-29T22:12:35.030782800Z",
          "start_time": "2024-12-29T22:12:35.015125900Z"
        },
        "id": "93XBgHJIFJ10"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWfZ2H3rFJ10"
      },
      "source": [
        "#### Initialize the agent and start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-29T22:12:37.853226900Z",
          "start_time": "2024-12-29T22:12:35.030782800Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EtWjP0RFJ10",
        "outputId": "3e9628d4-9e99-4065-db92-cc0bec09630c"
      },
      "outputs": [],
      "source": [
        "\n",
        "from common.utils import mini_batch_train\n",
        "from ddpg.ddpg import DDPGAgent\n",
        "\n",
        "#initialize environment\n",
        "env = PendulumEnv() # alternative: gym.make(\"Pendulum-v0\")\n",
        "\n",
        "max_episodes = 10  #total number of episodes to stop training after\n",
        "max_steps = 100 #max number of steps per episode\n",
        "batch_size = 300 #batch size for updates\n",
        "buffer_maxlen = 100000 #max buffer size\n",
        "\n",
        "# define training hyperparameters\n",
        "gamma = 0.99 #discount factor\n",
        "tau = 1e-2  #for updates with target network\n",
        "\n",
        "critic_lr = 1e-3 #learning rate\n",
        "actor_lr = 1e-3 #learning rate\n",
        "\n",
        "#initialize agent\n",
        "agent = DDPGAgent(env, gamma, tau, buffer_maxlen, critic_lr, actor_lr, max_action = 2) #max_action is set by what the environment expects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "eQGEOh41FJ11",
        "outputId": "082e3b9e-2f1f-4fb3-eb62-ae0516072a85"
      },
      "outputs": [],
      "source": [
        "#train with mini-batches and plotting for 10 episodes\n",
        "max_episodes = 5\n",
        "episode_rewards, env = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hNhpC5UFJ11"
      },
      "source": [
        "#### Question: how does the agent performance look early in training? What would you expect to see?\n",
        "Now continue trainining without plotting (plotting slows down execution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQc1ii_LFJ11",
        "outputId": "c8135345-1e58-4039-9202-24668f15d6e4"
      },
      "outputs": [],
      "source": [
        "#faster training without plotting for 50 more episodes\n",
        "env.plot = False\n",
        "max_episodes = 50\n",
        "episode_rewards, env = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "yb8xQW-UFJ12",
        "outputId": "fd323457-331c-475f-927c-da52cc0f13b0"
      },
      "outputs": [],
      "source": [
        "#plot the results of that previous training\n",
        "env.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff7haYkCFJ12"
      },
      "source": [
        "#### Question: how does the agent performance look as training progresses? What would you expect to see?\n",
        "Now examine the agent's behavior at present for 5 episodes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "AbFkpJjkFJ12",
        "outputId": "8923adec-edc6-4bb6-8e15-3fd609aff0a4"
      },
      "outputs": [],
      "source": [
        "#rest plot and see how training is going with live plotting\n",
        "env.reset_plot()\n",
        "env.plot = True\n",
        "max_episodes = 5\n",
        "episode_rewards, env = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkZPl0_fFJ12",
        "outputId": "3b71ffad-11df-4cbd-83dd-d567a8578f9c"
      },
      "outputs": [],
      "source": [
        "#continue training\n",
        "env.plot = False\n",
        "max_episodes = 90\n",
        "episode_rewards, env = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-12-29T22:19:53.655157100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "YXk1ie9yFJ12",
        "outputId": "8ef52714-c74e-46b6-d224-19570b137f1d"
      },
      "outputs": [],
      "source": [
        "env.plot_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-12-29T22:19:53.655157100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "2Kr1OlotFJ13",
        "outputId": "308bbada-0676-4fee-cc45-a1491f7f1fb3"
      },
      "outputs": [],
      "source": [
        "#rest plot and see how training is going with live plotting\n",
        "env.reset_plot()\n",
        "env.plot = True\n",
        "max_episodes = 5\n",
        "episode_rewards, env = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxAagdT-FJ13"
      },
      "source": [
        "#### Question: how does the agent performance look after a lot of training?\n",
        " How do the actions it takes and the reward obtained in each episode compare to early on in training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd39DZDiFJ13"
      },
      "source": [
        "\n",
        "<a id='beamsize'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPVsyTvKj0ie"
      },
      "source": [
        "#### Question: based on your observations, please comment on what the pros and cons of using RL for online accelerator tuning might be, compared to other approaches we have studied in the class?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
