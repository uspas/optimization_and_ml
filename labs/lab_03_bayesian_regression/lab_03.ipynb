{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwXV1t5Rov1v"
   },
   "source": [
    "# Lab 03 - Gaussian Process Modeling\n",
    "## Tasks\n",
    "- Construct a Gaussian Process model using GPyTorch and tune hyperparameters of GP model given noisy data\n",
    "- Construct Gaussian Process models using the Xopt package\n",
    "- Gaussian Process model visualization and sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urfcN6v_ov1w"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install botorch==0.12.0 gpytorch xopt==2.5.2"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYpEBMB7o5CC",
    "outputId": "8b5ed0d8-7b26-4542-d08d-7ac88b8e1f89"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLwRZCDgov1x"
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "import torch\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gaussian Process modeling"
   ],
   "metadata": {
    "id": "c3OioqGGQKru"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sz6MpV6vov1x"
   },
   "source": [
    "## Generate Data (1D)\n",
    "We are going to look at some data generated by random sampling in the domain [0,1]. The function that generated this data is\n",
    "\n",
    "$$\n",
    "f(x) = \\sin(2\\pi x) + x\n",
    "$$\n",
    "\n",
    "The columns of the array is $(x)$. We need to convert it to a torch tensor to use with GPyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "id": "EeFlf8U7ov1y"
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(5)\n",
    "train_x = x.reshape(-1,1)\n",
    "train_y = np.sin(2*np.pi*train_x[:,0]) + train_x[:,0] + np.random.randn(train_x.shape[0]) * 0.01\n",
    "\n",
    "train_x = torch.from_numpy(train_x)\n",
    "train_y = torch.from_numpy(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WujfYndlov1y"
   },
   "source": [
    "## Define a GP Model in GPyTorch\n",
    "Here we define an Exact GP model using GPyTorch. The model is exact because we have analytic expressions for the integrals associated with the GP likelihood and output distribution. If we had a non-Gaussian likelihood or some other complication that prevented analytic integration we can also use Variational/Approximate/MCMC techniques to approximate the integrals necessary.\n",
    "\n",
    "Taking a close look at the model below we see two important modules:\n",
    "- ```self.mean_module``` which represents the mean function\n",
    "- ```self.covar_module``` which represents the kernel function (or what is used to calculate the kernel matrix\n",
    "\n",
    "Both of these objects are torch.nn.Module objects (see https://pytorch.org/docs/stable/generated/torch.nn.Module.html). PyTorch modules have trainable parameters which we can access when doing training. By grouping the modules inside another PyTorch module (gpytorch.models.ExactGP) lets us easily control which parameters are trained and which are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjsFjD7dov1z"
   },
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_f, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_f, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvUpgu15ov1z"
   },
   "source": [
    "Here we initialize our model with the training data and a defined likelihood (also a nn.Module) with a trainable noise parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RREkpLoYov10"
   },
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srmZIarjov10"
   },
   "source": [
    "NOTE: All PyTorch modules (including ExactGPModel) have ```.train()``` and ```.eval()``` modes. ```train()``` mode is for optimizing model hyperameters. ```.eval()``` mode is for computing predictions through the model posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31HAqzmwov10"
   },
   "source": [
    "## Training the model\n",
    "Here we train the hyperparameters of the model (the parameters of the covar_module and the mean_module) to maximize the marginal log likelihood (minimize the negative marginal log likelihood). Note that since everything is defined in pyTorch we can use Autograd functionality to get the derivatives which will speed up optimization using the modified gradient descent algorithm ADAM.\n",
    "\n",
    "Also note that several of these hyperparameters (lengthscale and noise) must be strictly positive. Since ADAM is an unconstrained optimizer (which optimizes over the domain (-inf, inf)) gpytorch accounts for this constraint by optimizing the log of the lengthscale (raw_lengthscale). To get the actual lengthscale just use ```model.covar_module.base_kernel.lengthscale.item()```\n",
    "\n",
    "### **Task:**\n",
    "\n",
    "Write the steps for minimizing the negative log likelihood using pytorch. Refer back to Lab 1 for a reminder of how to do this. Use `gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)` as the loss function (which we are trying to maximize!). Use your function to train the model and report the marginal log likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95Zpp_deov10"
   },
   "outputs": [],
   "source": [
    "def train_model(model, likelihood):\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    #print the new trainable parameters\n",
    "    for param in model.named_parameters():\n",
    "        print(f'{param[0]} : {param[1]}')\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3nH6FYuov11",
    "outputId": "67ccd891-b4d5-4ecf-80ec-0eaf4fc254f5"
   },
   "outputs": [],
   "source": [
    "nmll = train_model(model, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I4i4P4sov11"
   },
   "source": [
    "## Plot the 1D model probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "dIuEZZsDov11",
    "outputId": "5615bc69-6cac-4b92-bcb8-070b04b3cfa0"
   },
   "outputs": [],
   "source": [
    "#plot the gp distribution in the normalized range\n",
    "x = torch.linspace(0, 1, 50).double()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(x)\n",
    "\n",
    "    #get the mean\n",
    "    m = p.mean\n",
    "\n",
    "    #get the 2 sigma confidence region around the mean\n",
    "    l,u = p.confidence_region()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "#plot mean and confidence region\n",
    "ax.plot(x, m)\n",
    "ax.fill_between(x.squeeze(), l, u, alpha = 0.25, lw = 0)\n",
    "\n",
    "#plot samples\n",
    "ax.plot(train_x, train_y,'oC1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "id": "KMEb8wGnov12"
   },
   "source": [
    "## Plot the samples from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "xDj61_sXov12",
    "outputId": "ad5674c0-e446-4ad9-9c90-34d649195d39"
   },
   "outputs": [],
   "source": [
    "#use the normalized range\n",
    "x = torch.linspace(0, 1, 50).double()\n",
    "#specify number of samples\n",
    "n_samples = 10\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(x)\n",
    "    s = p.rsample(torch.Size([n_samples]))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "#plot samples from posterior model\n",
    "for sample in s:\n",
    "    ax.plot(x, sample,'C0',alpha = 0.5)\n",
    "\n",
    "#plot measurements\n",
    "ax.plot(train_x, train_y,'oC1')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building and visualizing models in Xopt\n",
    "Xopt builds models in Botorch, which has a separate class for GP modeling that loosely wraps ExactGP classes. Visualizing the model has some slight differences."
   ],
   "metadata": {
    "collapsed": false,
    "id": "FLu-2sHFov12"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xopt.generators.bayesian.models.standard import StandardModelConstructor\n",
    "from xopt import VOCS\n",
    "import pandas as pd\n",
    "\n",
    "vocs = VOCS(variables={\"x\":[0,1]}, observables=[\"y\"])\n",
    "data = pd.DataFrame({\"x\":train_x.flatten().numpy(), \"y\":train_y.flatten().numpy()})\n",
    "\n",
    "# define a model constructor\n",
    "model_constructor = StandardModelConstructor()\n",
    "xopt_gp_model = model_constructor.build_model_from_vocs(\n",
    "    vocs=vocs,\n",
    "    data=data,\n",
    ")"
   ],
   "metadata": {
    "id": "yXOtEK3Vov12"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xopt.generators.bayesian.visualize import visualize_model\n",
    "visualize_model(xopt_gp_model, vocs, data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "QeCh-jmIov13",
    "outputId": "4bc5317a-7fd7-4eb5-d390-b5755d711357"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOp9YeJUov13"
   },
   "source": [
    "## Generate Data (3D)\n",
    "We are going to look at some data that was generated by sampling a 5 x 5 x 5 grid in the domain [0,1] on each axis. The function that generated this data is\n",
    "\n",
    "$$\n",
    "f(x_1,x_2,x_3) = \\sin(2\\pi x_1)\\sin(\\pi x_2) + x_3\n",
    "$$\n",
    "\n",
    "The columns of the imported array is $(x_1,x_2,x_3,f)$. We need to convert it to a torch tensor to use with GPyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIZg7hWBov13"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,5)\n",
    "xx = np.meshgrid(x,x,x)\n",
    "train_x = np.vstack([ele.ravel() for ele in xx]).T\n",
    "train_y = np.sin(2*np.pi*train_x[:,0]) * np.sin(np.pi*train_x[:,1]) + train_x[:,2] + np.random.randn(train_x.shape[0]) * 0.01\n",
    "\n",
    "train_x = torch.from_numpy(train_x)\n",
    "train_y = torch.from_numpy(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3G7zdryWov13"
   },
   "source": [
    "### **Task:**\n",
    "Define a new GP model that uses a different kernel (or combination of kernels) to maximize the marginal log likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYFdgr2cov13"
   },
   "outputs": [],
   "source": [
    "class MyExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MyExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4s1D0B_ov13",
    "outputId": "f032eb8f-28b5-4f29-aae8-99b415fa121d"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCiz9njBov14"
   },
   "source": [
    "### Use the code below to visualize the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZSu0wNEov14",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "036e8622-a3f6-49cb-ed4e-bd232c9be640"
   },
   "outputs": [],
   "source": [
    "#Hint: you can use the following code to get the points to be evaluated\n",
    "import torch\n",
    "n = 50\n",
    "x = torch.meshgrid(torch.linspace(0,1,n), torch.linspace(0,1,n))\n",
    "pts = torch.vstack([ele.flatten() for ele in x]).T\n",
    "pts = torch.hstack((pts, torch.zeros((n**2,1)))).unsqueeze(1)\n",
    "\n",
    "pts.shape"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# evaluate the model and get the mean + variance\n",
    "my_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    post = my_likelihood(my_model(pts))\n",
    "    mean = post.mean\n",
    "    variance = post.variance"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PXlJ2PPAGnu",
    "outputId": "59fd5808-556a-4a6a-98fe-d9ff08e2284a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# plot the model\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolor(*x, mean.reshape(n,n))\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolor(*x, variance.reshape(n,n))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "rrkvAwcsAy2g",
    "outputId": "7375644d-3690-4159-d7e9-73861bb55f89"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
