{"cells":[{"cell_type":"markdown","metadata":{"id":"jQjIBIgDKNOv"},"source":["# Lab 06 - Uncertainty Quantification\n","\n","## Tasks\n","\n","- Train different machine learning algorithms on noisy data\n","- Predict the uncertainty from the trained model"]},{"cell_type":"markdown","metadata":{"id":"3tiqkaL7KNOw"},"source":["## Set up environment"]},{"cell_type":"code","source":["%reset -f\n","!pip install botorch\n","!pip install cheetah-accelerator\n","!pip install blitz-bayesian-pytorch"],"metadata":{"id":"bi7U2KQJP6IH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKPiifUlKNOw"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tqdm # For progress bar\n","\n","# Gaussian process\n","from botorch.models import SingleTaskGP\n","from gpytorch.mlls import ExactMarginalLogLikelihood\n","from botorch.fit import fit_gpytorch_mll\n","from botorch.models.transforms import Normalize, Standardize\n","\n","\n","# Neural networks\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"]},{"cell_type":"markdown","metadata":{"id":"IUF0TXVOKNOy"},"source":["## Create training data\n","\n","We start by generating noisy data from a model of beam propagating through a single quad magnet (characterized by a magnetic strength `K1`). In order to simulate fluctuations in beam parameters and noise in the beam size measurement, we add noise in the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBiHCw9CKNOy"},"outputs":[],"source":["from cheetah.particles import ParameterBeam\n","from cheetah.accelerator import Drift, Quadrupole, Segment\n","\n","incoming_beam = ParameterBeam.from_twiss(\n","    beta_x=torch.tensor(5.0),\n","    alpha_x=torch.tensor(0.0),\n","    emittance_x=torch.tensor(1e-8)\n",")\n","\n","beamline = Segment(\n","    [\n","        Drift(length=torch.tensor(1.0)),\n","        Quadrupole(name=\"Q1\",length=torch.tensor(0.1)),\n","        Drift(length=torch.tensor(1.0))\n","    ]\n",")\n","\n","def calculate_x_beamsize(K, add_noise=False):\n","    # set beamline parameters\n","    beamline.Q1.k1 = K\n","\n","    # track the beam\n","    final_beam = beamline.track(incoming_beam)\n","\n","    # noise\n","    if add_noise:\n","        noise = torch.randn_like(K) * 1e-1\n","    else:\n","        noise = 0.0\n","\n","    # return the beam size in x (in mm)\n","    return final_beam.sigma_x * 1e3 + noise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KD0vp51uKNOy"},"outputs":[],"source":["# Generate 400 samples for K1 varying between -300 and 300\n","n_samples = 400\n","bounds = torch.tensor([-30.0, 30.0])\n","train_x = torch.rand(n_samples) * (bounds[1] - bounds[0]) + bounds[0]\n","train_x = train_x.reshape(-1,1)\n","train_y = torch.cat([calculate_x_beamsize(train_x, add_noise=True)]).detach()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLIqB7czKNOy"},"outputs":[],"source":["plt.plot(train_x, train_y, '.' )\n","plt.xlabel('Magnetic strength K1')\n","plt.ylabel('Beam size')"]},{"cell_type":"markdown","metadata":{"id":"a3FQC94DKNOy"},"source":["# Gaussian process\n","\n","Here we fit a Gaussian process to the data, we draw a few samples from the GP and plot them, along with the uncertainty."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDAZhNmaKNOy"},"outputs":[],"source":["# Define a Gaussian Process model with Matern(5/2) kernel and Gaussian likelihood\n","gp = SingleTaskGP(\n","    train_x.double(), train_y.double(),\n","    input_transform=Normalize(d=1),\n","    outcome_transform=Standardize(m=1),\n",")\n","\n","# Train model hyperparameters by minimizing negative-Log-Marginal-Likelihood\n","mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n","fit_gpytorch_mll(mll);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLlvIWwJKNOz"},"outputs":[],"source":["# Plot the original training data\n","plt.plot(train_x, train_y, '.', alpha=0.3)\n","\n","# Draw samples of the GP, and plot them over the interval -0.5, 1.5\n","x = torch.linspace(-50.0, 50.0, 200).reshape(-1,1)\n","p = gp.posterior(x, observation_noise=False)\n","\n","for _ in range(10):\n","    sample, = p.rsample()\n","    plt.plot( x.flatten(), sample.detach().numpy().flatten(), alpha=0.5 )\n","\n","# Plot the analytical uncertainty\n","m = p.mean\n","l,u = p.mvn.confidence_region()\n","plt.fill_between( x.flatten(), l.detach().numpy(), u.detach().numpy(), color='b', alpha=0.2)\n","\n","plt.xlabel('Normalized magnetic strength K1')\n","plt.ylabel('Normalized beam size')"]},{"cell_type":"markdown","metadata":{"id":"T_nkAqtjKNOz"},"source":["> ### **Task:**\n","> Does the uncertainty behave as expected? In which way? Does the predicted uncertainty capture the aleatoric part or only the epistemic part? Copy the above code below, and change `observation_noise` from `False` to `True`. Does it now capture the aleatoric part or only the epistemic part?"]},{"cell_type":"markdown","metadata":{"id":"QVCUN1h0KNOz"},"source":["> **Your answer here** (Does the uncertainty behave as expected? In which way? Does the predicted uncertainty capture the aleatoric part or only the epistemic part?)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3p6iPPjvKNOz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJuCLeeRKNO0"},"outputs":[],"source":["# Your code here: Copy the above code below, and change`observation_noise` from `False` to `True`.\n","\n"]},{"cell_type":"markdown","source":["> **Your answer here** (Does it now capture the aleatoric part or only the epistemic part?)"],"metadata":{"id":"uauQoHe098vf"}},{"cell_type":"code","source":[],"metadata":{"id":"CmJL6rw1-Azg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yLnqlol3KNO0"},"source":["# Ensemble of neural networks\n","\n","We will now train an **ensemble** of neural networks on the same data."]},{"cell_type":"markdown","source":["> ### **Task:**\n","> Implement a neural network with 2 hidden layers of 10 neurons each.\n","> Train 10 randomly initialized models on the same data. Store the trained models in a list."],"metadata":{"id":"gNdyKWHrhUmh"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# normalize training data\n","normed_train_x = ((train_x - train_x.min()) / (train_x.max() - train_x.min())).float()\n","normed_train_y = ((train_y - train_y.min()) / (train_y.max() - train_y.min())).float()"],"metadata":{"id":"hJ9jX6gHKNO0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_SLqEyFKNO0"},"outputs":[],"source":["class NNModel(nn.Module):\n","    \"\"\"\n","    Define fully-connected neural network with 2 hidden layers\n","    \"\"\"\n","    # Your code here: implement a neural network with 2 hidden layers of 10 neurons each."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9at4FpzoKNO0"},"outputs":[],"source":["def generate_and_train_model( epochs=100, plot_loss=True ):\n","    \"\"\"\n","    Train a neural network, optionally plot the loss during training\n","    (to check convergence), and return the corresponding neural network object\n","    \"\"\"\n","    # Your code here: implement creation and training of one neural network model\n","\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkBjHjvOKNO0"},"outputs":[],"source":["# Train 10 different neural network on the same data\n","ensemble_models = [ generate_and_train_model() for _ in range(10) ]"]},{"cell_type":"code","source":["def plot_samples_with_mean_plusminus_std(x, y_samples, show_samples=True):\n","    \"\"\"\n","    Helper function that plots samples, along with the mean +/- std\n","\n","    x: numpy array of size (n,)\n","        points on the horizontal axis\n","\n","    y_samples: list of numpy arrays of size (n,)\n","        the corresponding samples\n","\n","    show_samples: bool\n","        whether to show the samples or only the mean and std\n","    \"\"\"\n","    # Plot samples\n","    if show_samples:\n","        for i in range(len(y_samples)):\n","            plt.plot(x, y_samples[i], alpha=0.5)\n","\n","    # Calculate mean and std\n","    y = np.array(y_samples)\n","    mean = np.mean(y,axis=0)\n","    std = np.std(y,axis=0)\n","    plt.plot( x, mean, 'b' )\n","    plt.fill_between( x, mean-std, mean+std, color='b', alpha=0.2)"],"metadata":{"id":"Y_iPnfnCAYWL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u00qA7lVKNO0"},"source":["> ### **Task:**\n","> Modify the code below to plot sample predictions from the ensemble of neural network, and to plot the mean and uncertainty. Since each neural network has been trained on the same dataset, why do they give different predictions? Does the resulting uncertainty capture the aleatoric part?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVmyjSJOKNO0"},"outputs":[],"source":["# Plot the original training data\n","plt.plot( normed_train_x, normed_train_y, '.', alpha=0.3 )\n","\n","# Plot predictions of the neural networks over the interval -0.5, 1.5\n","x = torch.linspace(-0.5, 1.5, 200).reshape(-1,1)\n","\n","y_samples = []\n","for model in ensemble_models:\n","    # Your code here: capture the prediction of each neural network\n","\n","\n","\n","\n","\n","plot_samples_with_mean_plusminus_std(x.flatten(), y_samples, show_samples=True)\n","\n","plt.xlabel('Normalized magnetic strength K1')\n","plt.ylabel('Normalized beam size')\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pDEG3vN1KNO1"},"source":["> **Your answer here:** (Since each neural network has been trained on the same dataset, why do they give different predictions? Does the resulting uncertainty capture the aleatoric part?)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XfOhlqiQKNO1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"SqXQcSDdKNO1"},"source":["# Monte-Carlo Dropout\n","\n","We now use a Monte-Carlo drop-out neural network to try to assess the uncertainty."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwbLZRQSKNO1"},"outputs":[],"source":["class MCDropoutModel(nn.Module):\n","\n","    def __init__(self, p):\n","        \"\"\"\n","        Initialize a Monte-Carlo drop-out neural network\n","\n","        p: float\n","            Drop-out probability\n","        \"\"\"\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.Linear(1, 10),\n","            nn.ReLU(),\n","            nn.Dropout(p), # Set activations to 0 with probability p\n","            nn.Linear(10, 10),\n","            nn.ReLU(),\n","            nn.Dropout(p), # Set activations to 0 with probability p\n","            nn.Linear(10, 10),\n","            nn.ReLU(),\n","            nn.Dropout(p), # Set activations to 0 with probability p\n","            nn.Linear(10, 1),\n","        )\n","\n","    def forward(self, x_train):\n","        x = self.layer(x_train)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NokcHv1-KNO1"},"outputs":[],"source":["def generate_and_train_dropout_model( p=0.1, epochs=100, plot_loss=True ):\n","    \"\"\"\n","    Train an MC drop-out neural network, optionally plot the loss during training\n","    (to check convergence), and return the corresponding neural network object\n","    \"\"\"\n","    model = MCDropoutModel(p)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=2e-2)\n","\n","    loss_list = []\n","    for epoch in tqdm.tqdm(range(epochs)):\n","        output = model(normed_train_x)\n","        loss = nn.MSELoss()(normed_train_y, output)\n","        loss_list.append( float(loss) )\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if plot_loss:\n","        plt.plot( loss_list )\n","        plt.xlabel( 'Epoch' )\n","        plt.ylabel( 'Training loss' )\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlqjJJKDKNO1"},"outputs":[],"source":["# Train the neural network in the presence of Drop-out\n","mcdropout_model = generate_and_train_dropout_model(epochs=200)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0LzFnAiKNO1"},"outputs":[],"source":["# Execute this cell several times.\n","# The result changes everytime, because different activation neurons are set to 0 (with probability p).\n","# This gives the uncertainty on the prediction.\n","mcdropout_model( torch.tensor([[0.3]]) )"]},{"cell_type":"markdown","metadata":{"id":"5xFdNsLpKNO1"},"source":["> ### **Task:**\n","> Generate 10 different sample predictions with the MC dropout model, and plot the samples with their corresponding uncertainty."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4Dg1ftDKNO1"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","metadata":{"id":"miIcf4OwKNO2"},"source":["> ### **Task:**\n","> Execute the cell below, and plot samples with uncertainty bands for the model. What happens and why?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRMglZfcKNO2"},"outputs":[],"source":["mcdropout_model.eval(); # Set in evaluation mode (this can be reverted by doing `mcdropout_model.train()`)"]},{"cell_type":"code","source":[],"metadata":{"id":"W8YudCyrbnN3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9W1IwkBKNO2"},"source":["> **Your answer here:** (What happens and why?)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9ukLqRbKNO2"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5fJogJEsKNO2"},"source":["# Quantile regression\n","\n","We will now evaluate the uncertainty using quantile regression.\n","\n","In quantile regression, we instantiate a regular neural network, but train it with specific loss function (see the slides) instead of the usual Mean Squared Error loss function."]},{"cell_type":"markdown","metadata":{"id":"kMdN7e5SKNO2"},"source":["> ### **Task:**\n","> Implement the quantile regression loss function in the cell below, and execute the following cells to fit quantile regression models to the data. Hint: use `torch.where`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQg2GjtmKNO2"},"outputs":[],"source":["def quantile_loss_function(y_true, y_pred, tau):\n","    \"\"\"\n","    Return the loss function for quantile regression\n","\n","    y_true: array of shape (n,1)\n","        data labels\n","\n","    y_pred: array of shape (n,1)\n","        predictions of the machine learning model\n","\n","    tau: float\n","        (number between 0 and 1)\n","    \"\"\"\n","    # Your code below: modify the lines below to defined the quantile loss function\n","    # (The current code implements the MSE loss function instead.)\n","\n","    error = y_true - y_pred\n","    #loss = torch.mean( error**2, axis=0 )\n","\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VllYJCDCKNO2"},"outputs":[],"source":["def generate_and_train_quantile_model( tau, epochs=100, plot_loss=True ):\n","    \"\"\"\n","    Train an quantile neural network, optionally plot the loss during training\n","    (to check convergence), and return the corresponding neural network object\n","\n","    quantile: float\n","        (number between 0 and 1)\n","    \"\"\"\n","    model = NNModel()\n","\n","    optimizer = optim.Adam(model.parameters(), lr=2e-2)\n","\n","    loss_list = []\n","    for epoch in tqdm.tqdm(range(epochs)):\n","        output = model(normed_train_x)\n","        loss = quantile_loss_function(normed_train_y, output, tau=tau)\n","        loss_list.append( float(loss) )\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if plot_loss:\n","        plt.plot( loss_list )\n","        plt.xlabel( 'Epoch' )\n","        plt.ylabel( 'Training loss' )\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CrL0VxxlKNO2"},"outputs":[],"source":["# Pick a few quantile values, and train a neural networks for each of them\n","tau_values = [0.2, 0.5, 0.9]\n","quantile_models = [ generate_and_train_quantile_model(tau, epochs=200) for tau in tau_values ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-yv5f6bKNO2"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"FUlFrpL6KNO3"},"source":["> ### **Task:**\n","> Are the relative positions of the 0.2, 0.5 and 0.9 curves as expected? Does this capture the aleatoric part of the uncertainty?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTxebAjOKNO3"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"7UbOViuHKNO3"},"source":["# Bayesian neural networks\n","\n","We will now use the `blitz` package to train bayesian neural networks. `blitz` combines with standard `pytorch` neural network models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4pOJNT0KNO3"},"outputs":[],"source":["# Bayesian neural network\n","from blitz.modules import BayesianLinear\n","from blitz.utils import variational_estimator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecgbRxDEKNO4"},"outputs":[],"source":["# Define a bayesian neural network with two hidden layers (with 10 neurons each)\n","\n","# - We use `BayesianLinear` instead of `Linear`.\n","#     `BayesianLinear` holds `mu` and `rho` (for each neuron) as trainable parameter, instead of the weights directly.\n","#     Its output is random, as it draws random weights for each evaluation\n","# - `variational_estimator` enables additional methods for the class, including the ELBO loss function\n","\n","@variational_estimator\n","class BayesianModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            BayesianLinear(1, 10, prior_sigma_1=1., prior_sigma_2=1.e-6, prior_pi=0.5),\n","            nn.ReLU(),\n","            BayesianLinear(10, 10, prior_sigma_1=1., prior_sigma_2=1.e-6, prior_pi=0.5),\n","            nn.ReLU(),\n","            BayesianLinear(10, 1, prior_sigma_1=1., prior_sigma_2=1.e-6, prior_pi=0.5),\n","        )\n","\n","    def forward(self, x_train):\n","        x = self.layer(x_train)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7mMfQk5KNO4"},"outputs":[],"source":["def generate_and_train_bayesian_model( epochs=1000, plot_loss=True ):\n","    \"\"\"\n","    Train a Bayesian neural network, optionally plot the loss during training\n","    (to check convergence), and return the corresponding neural network object\n","    \"\"\"\n","    model = BayesianModel()\n","\n","    optimizer = optim.Adam(model.parameters(), lr=2e-2)\n","\n","    loss_list = []\n","    for epoch in tqdm.tqdm(range(epochs)):\n","\n","        loss = model.sample_elbo(inputs=normed_train_x,\n","                                 labels=normed_train_y,\n","                                 criterion=nn.MSELoss(),\n","                                 sample_nbr=2,\n","                                 complexity_cost_weight=1./normed_train_x.shape[0])\n","        loss_list.append( float(loss) )\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if plot_loss:\n","        plt.plot( loss_list )\n","        plt.xlabel( 'Epoch' )\n","        plt.ylabel( 'Training loss' )\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ad53LuILKNO4"},"outputs":[],"source":["# Train the neural network\n","bayesian_model = generate_and_train_bayesian_model()"]},{"cell_type":"markdown","metadata":{"id":"NAay60I-KNO4"},"source":["> ### **Task:**\n","> By modifying the code below, plot the uncertainty for the Bayesian neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YeHJW_vNKNO4"},"outputs":[],"source":["# Plot the original training data\n","plt.plot( normed_train_x, normed_train_y, '.', alpha=0.3 )\n","\n","# Plot predictions of the neural networks over the interval -0.5, 1.5\n","x = torch.linspace(-0.5, 1.5, 200).reshape(-1,1)\n","\n","y_samples = []\n","# Your code here: Collect 10 different sample predictions\n","\n","\n","\n","plot_samples_with_mean_plusminus_std(x.flatten(), y_samples, show_samples=True)\n","\n","plt.xlabel('Normalized magnetic strength K1')\n","plt.ylabel('Normalized beam size')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aVBZRtkKNO4"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}