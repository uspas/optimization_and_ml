{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQjIBIgDKNOv"
   },
   "source": [
    "# Lab 06 - Uncertainty Quantification\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- Train different machine learning algorithms on noisy data\n",
    "- Predict the uncertainty from the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tiqkaL7KNOw"
   },
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%reset -f\n",
    "!pip install botorch\n",
    "!pip install cheetah-accelerator\n",
    "!pip install blitz-bayesian-pytorch"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bi7U2KQJP6IH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284584199,
     "user_tz": 360,
     "elapsed": 148143,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    },
    "outputId": "5eec8191-79cd-49cc-f0ff-7adc04626e78"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKPiifUlKNOw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm # For progress bar\n",
    "\n",
    "# Gaussian process\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.models.transforms import Normalize, Standardize\n",
    "\n",
    "\n",
    "# Neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUF0TXVOKNOy"
   },
   "source": [
    "## Create training data\n",
    "\n",
    "We start by generating noisy data from a model of beam propagating through a single quad magnet (characterized by a magnetic strength `K1`). In order to simulate fluctuations in beam parameters and noise in the beam size measurement, we add noise in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBiHCw9CKNOy"
   },
   "outputs": [],
   "source": [
    "from cheetah.particles import ParameterBeam\n",
    "from cheetah.accelerator import Drift, Quadrupole, Segment\n",
    "\n",
    "incoming_beam = ParameterBeam.from_twiss(\n",
    "    beta_x=torch.tensor(5.0),\n",
    "    alpha_x=torch.tensor(0.0),\n",
    "    emittance_x=torch.tensor(1e-8)\n",
    ")\n",
    "\n",
    "beamline = Segment(\n",
    "    [\n",
    "        Drift(length=torch.tensor(1.0)),\n",
    "        Quadrupole(name=\"Q1\",length=torch.tensor(0.1)),\n",
    "        Drift(length=torch.tensor(1.0))\n",
    "    ]\n",
    ")\n",
    "\n",
    "def calculate_x_beamsize_squared(K, add_noise=False):\n",
    "    # set beamline parameters\n",
    "    beamline.Q1.k1 = K\n",
    "\n",
    "    # track the beam\n",
    "    final_beam = beamline.track(incoming_beam)\n",
    "\n",
    "    # noise\n",
    "    if add_noise:\n",
    "        noise = torch.randn_like(K) * 1e-1\n",
    "    else:\n",
    "        noise = 0.0\n",
    "\n",
    "    # return the beam size in x (in mm)\n",
    "    return final_beam.sigma_x**2 * 1e6 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KD0vp51uKNOy"
   },
   "outputs": [],
   "source": [
    "# Generate 400 samples for K1 varying between -300 and 300\n",
    "n_samples = 400\n",
    "bounds = torch.tensor([-30.0, 30.0])\n",
    "train_x = torch.rand(n_samples) * (bounds[1] - bounds[0]) + bounds[0]\n",
    "train_x = train_x.reshape(-1,1)\n",
    "train_y = torch.cat([calculate_x_beamsize_squared(train_x, add_noise=True)]).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLIqB7czKNOy",
    "outputId": "873723d8-5752-4f1e-c7f6-d628a9a1be57",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284616242,
     "user_tz": 360,
     "elapsed": 1492,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_x, train_y, '.' )\n",
    "plt.xlabel('Magnetic strength K1')\n",
    "plt.ylabel('Beam size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3FQC94DKNOy"
   },
   "source": [
    "# Gaussian process\n",
    "\n",
    "Here we fit a Gaussian process to the data, we draw a few samples from the GP and plot them, along with the uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDAZhNmaKNOy"
   },
   "outputs": [],
   "source": [
    "# Define a Gaussian Process model with Matern(5/2) kernel and Gaussian likelihood\n",
    "gp = SingleTaskGP(\n",
    "    train_x.double(), train_y.double(),\n",
    "    input_transform=Normalize(d=1),\n",
    "    outcome_transform=Standardize(m=1),\n",
    ")\n",
    "\n",
    "# Train model hyperparameters by minimizing negative-Log-Marginal-Likelihood\n",
    "mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "fit_gpytorch_mll(mll);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLlvIWwJKNOz",
    "outputId": "c1fb9c02-b4cc-44ad-a106-ee18ece73588",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284621866,
     "user_tz": 360,
     "elapsed": 1769,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot the original training data\n",
    "plt.plot(train_x, train_y, '.', alpha=0.3)\n",
    "\n",
    "# Draw samples of the GP, and plot them over the interval -0.5, 1.5\n",
    "x = torch.linspace(-50.0, 50.0, 200).reshape(-1,1)\n",
    "p = gp.posterior(x, observation_noise=False)\n",
    "\n",
    "for _ in range(10):\n",
    "    sample, = p.rsample()\n",
    "    plt.plot( x.flatten(), sample.detach().numpy().flatten(), alpha=0.5 )\n",
    "\n",
    "# Plot the analytical uncertainty\n",
    "m = p.mean\n",
    "l,u = p.mvn.confidence_region()\n",
    "plt.fill_between( x.flatten(), l.detach().numpy(), u.detach().numpy(), color='b', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Normalized magnetic strength K1')\n",
    "plt.ylabel('Normalized beam size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_nkAqtjKNOz"
   },
   "source": [
    "> ### **Task:**\n",
    "> Does the uncertainty behave as expected? In which way? Does the predicted uncertainty capture the aleatoric part or only the epistemic part? Copy the above code below, and change `observation_noise` from `False` to `True`. Does it now capture the aleatoric part or only the epistemic part?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVCUN1h0KNOz"
   },
   "source": [
    "> **Your answer here** (Does the uncertainty behave as expected? In which way? Does the predicted uncertainty capture the aleatoric part or only the epistemic part?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3p6iPPjvKNOz",
    "outputId": "dacf9f57-7c44-4399-b4fc-cf07e426cb04",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284621866,
     "user_tz": 360,
     "elapsed": 2,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJuCLeeRKNO0",
    "outputId": "4416404f-c1cd-46b8-d1fc-1a1b0909a520",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284623736,
     "user_tz": 360,
     "elapsed": 1872,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Your code here: Copy the above code below, and change`observation_noise` from `False` to `True`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLnqlol3KNO0"
   },
   "source": [
    "# Ensemble of neural networks\n",
    "\n",
    "We will now train an **ensemble** of neural networks on the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### **Task:**\n",
    "> Implement a neural network with 2 hidden layers of 10 neurons each.\n",
    "> Normalize the training data and train 10 randomly initialized models on the same data. Store the trained models in a list."
   ],
   "metadata": {
    "id": "gNdyKWHrhUmh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_SLqEyFKNO0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# normalize training data\n"
   ],
   "metadata": {
    "id": "hJ9jX6gHKNO0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9at4FpzoKNO0"
   },
   "outputs": [],
   "source": [
    "def generate_and_train_model( epochs=100, plot_loss=True ):\n",
    "    \"\"\"\n",
    "    Train a neural network, optionally plot the loss during training\n",
    "    (to check convergence), and return the corresponding neural network object\n",
    "    \"\"\"\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkBjHjvOKNO0",
    "outputId": "efa10736-86a4-4d5e-bba5-7414b43c8b76",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284630359,
     "user_tz": 360,
     "elapsed": 6625,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Train 10 different neural network on the same data\n",
    "ensemble_models = [ generate_and_train_model() for _ in range(10) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u00qA7lVKNO0"
   },
   "source": [
    "> ### **Task:**\n",
    "> Plot sample predictions from the ensemble of neural networks, and the mean and uncertainty. Since each neural network has been trained on the same dataset, why do they give different predictions? Does the resulting uncertainty capture the aleatoric part?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVmyjSJOKNO0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ebd_EkLAKNO1",
    "outputId": "ad486c94-6b72-44cf-b788-d8ca5164361c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284630651,
     "user_tz": 360,
     "elapsed": 293,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDEG3vN1KNO1"
   },
   "source": [
    "> **Your answer here:** (Since each neural network has been trained on the same dataset, why do they give different predictions? Does the resulting uncertainty capture the aleatoric part?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfOhlqiQKNO1",
    "outputId": "f479dc03-06e7-4d0d-cac7-4f8153f442d7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284630652,
     "user_tz": 360,
     "elapsed": 4,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqXQcSDdKNO1"
   },
   "source": [
    "# Monte-Carlo Dropout\n",
    "\n",
    "We now use a Monte-Carlo drop-out neural network to try to assess the uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwbLZRQSKNO1"
   },
   "outputs": [],
   "source": [
    "class MCDropoutModel(nn.Module):\n",
    "\n",
    "    def __init__(self, p):\n",
    "        \"\"\"\n",
    "        Initialize a Monte-Carlo drop-out neural network\n",
    "\n",
    "        p: float\n",
    "            Drop-out probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(1, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p), # Set activations to 0 with probability p\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p), # Set activations to 0 with probability p\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p), # Set activations to 0 with probability p\n",
    "            nn.Linear(10, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_train):\n",
    "        x = self.layer(x_train)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NokcHv1-KNO1"
   },
   "outputs": [],
   "source": [
    "def generate_and_train_dropout_model( p=0.1, epochs=100, plot_loss=True ):\n",
    "    \"\"\"\n",
    "    Train an MC drop-out neural network, optionally plot the loss during training\n",
    "    (to check convergence), and return the corresponding neural network object\n",
    "    \"\"\"\n",
    "    model = MCDropoutModel(p)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "\n",
    "    loss_list = []\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        output = model(normed_train_x)\n",
    "        loss = nn.MSELoss()(normed_train_y, output)\n",
    "        loss_list.append( float(loss) )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.plot( loss_list )\n",
    "        plt.xlabel( 'Epoch' )\n",
    "        plt.ylabel( 'Training loss' )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlqjJJKDKNO1",
    "outputId": "c3fe54c4-6571-4d38-bef5-9634d6aad6f1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284631413,
     "user_tz": 360,
     "elapsed": 700,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Train the neural network in the presence of Drop-out\n",
    "mcdropout_model = generate_and_train_dropout_model(epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0LzFnAiKNO1",
    "outputId": "bdc07086-17ed-4711-b2eb-84baf7ac4795",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284631413,
     "user_tz": 360,
     "elapsed": 3,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell several times.\n",
    "# The result changes everytime, because different activation neurons are set to 0 (with probability p).\n",
    "# This gives the uncertainty on the prediction.\n",
    "mcdropout_model( torch.tensor([[0.3]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xFdNsLpKNO1"
   },
   "source": [
    "> ### **Task:**\n",
    "> Generate 10 different sample predictions with the MC dropout model, and plot the samples with their corresponding uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4Dg1ftDKNO1",
    "outputId": "77f2ff59-d662-42b8-b4c8-606c714352d0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284631740,
     "user_tz": 360,
     "elapsed": 329,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miIcf4OwKNO2"
   },
   "source": [
    "> ### **Task:**\n",
    "> Execute the cell below, and plot samples with uncertainty bands for the model. What happens and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRMglZfcKNO2"
   },
   "outputs": [],
   "source": [
    "mcdropout_model.eval(); # Set in evaluation mode (this can be reverted by doing `mcdropout_model.train()`)"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "W8YudCyrbnN3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284631953,
     "user_tz": 360,
     "elapsed": 216,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    },
    "outputId": "7b56376b-12e5-4523-e584-7c373e837eb7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9W1IwkBKNO2"
   },
   "source": [
    "> **Your answer here:** (What happens and why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9ukLqRbKNO2",
    "outputId": "4e13e774-1bf2-4145-ffff-95f410ec2908",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284631953,
     "user_tz": 360,
     "elapsed": 4,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fJogJEsKNO2"
   },
   "source": [
    "# Quantile regression\n",
    "\n",
    "We will now evaluate the uncertainty using quantile regression.\n",
    "\n",
    "In quantile regression, we instantiate a regular neural network, but train it with specific loss function (see the slides) instead of the usual Mean Squared Error loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMdN7e5SKNO2"
   },
   "source": [
    "> ### **Task:**\n",
    "> Implement the quantile regression loss function in the cell below, and execute the following cells to fit quantile regression models to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQg2GjtmKNO2"
   },
   "outputs": [],
   "source": [
    "def quantile_loss_function(y_true, y_pred, tau):\n",
    "    \"\"\"\n",
    "    Return the loss function for quantile regression\n",
    "\n",
    "    y_true: array of shape (n,1)\n",
    "        data labels\n",
    "\n",
    "    y_pred: array of shape (n,1)\n",
    "        predictions of the machine learning model\n",
    "\n",
    "    tau: float\n",
    "        (number between 0 and 1)\n",
    "    \"\"\"\n",
    "    # Your code below: modify the lines below to defined the quantile loss function\n",
    "    # (The current code implements the MSE loss function instead.)\n",
    "\n",
    "    error = y_true - y_pred\n",
    "    #loss = torch.mean( error**2, axis=0 )\n",
    "\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VllYJCDCKNO2"
   },
   "outputs": [],
   "source": [
    "def generate_and_train_quantile_model( tau, epochs=100, plot_loss=True ):\n",
    "    \"\"\"\n",
    "    Train an quantile neural network, optionally plot the loss during training\n",
    "    (to check convergence), and return the corresponding neural network object\n",
    "\n",
    "    quantile: float\n",
    "        (number between 0 and 1)\n",
    "    \"\"\"\n",
    "    model = NNModel()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "\n",
    "    loss_list = []\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        output = model(normed_train_x)\n",
    "        loss = quantile_loss_function(normed_train_y, output, tau=tau)\n",
    "        loss_list.append( float(loss) )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.plot( loss_list )\n",
    "        plt.xlabel( 'Epoch' )\n",
    "        plt.ylabel( 'Training loss' )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrL0VxxlKNO2",
    "outputId": "1ff83286-72f0-4770-f9fa-5af7ad16ae0e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284633503,
     "user_tz": 360,
     "elapsed": 1553,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Pick a few quantile values, and train a neural networks for each of them\n",
    "tau_values = [0.2, 0.5, 0.9]\n",
    "quantile_models = [ generate_and_train_quantile_model(tau, epochs=200) for tau in tau_values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-yv5f6bKNO2",
    "outputId": "5404a7f9-3e0a-4878-ea63-c3e4ef5333fb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284633795,
     "user_tz": 360,
     "elapsed": 295,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUlFrpL6KNO3"
   },
   "source": [
    "> ### **Task:**\n",
    "> Are the relative positions of the 0.2, 0.5 and 0.9 curves as expected? Does this capture the aleatoric part of the uncertainty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTxebAjOKNO3",
    "outputId": "d3c99099-e170-4a32-e748-d881e39c7991",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284633795,
     "user_tz": 360,
     "elapsed": 5,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbOViuHKNO3"
   },
   "source": [
    "# Bayesian neural networks\n",
    "\n",
    "We will now use the `blitz` package to train bayesian neural networks. `blitz` combines with standard `pytorch` neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4pOJNT0KNO3"
   },
   "outputs": [],
   "source": [
    "# Bayesian neural network\n",
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecgbRxDEKNO4"
   },
   "outputs": [],
   "source": [
    "# Define a bayesian neural network with two hidden layers (with 10 neurons each)\n",
    "\n",
    "# - We use `BayesianLinear` instead of `Linear`.\n",
    "#     `BayesianLinear` holds `mu` and `rho` (for each neuron) as trainable parameter, instead of the weights directly.\n",
    "#     Its output is random, as it draws random weights for each evaluation\n",
    "# - `variational_estimator` enables additional methods for the class, including the ELBO loss function\n",
    "\n",
    "@variational_estimator\n",
    "class BayesianModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            BayesianLinear(1, 10, prior_sigma_1=1., prior_sigma_2=1.e-6, prior_pi=0.5),\n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(10, 10, prior_sigma_1=1., prior_sigma_2=1.e-6, prior_pi=0.5),\n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(10, 1, prior_sigma_1=1., prior_sigma_2=1.e-6, prior_pi=0.5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_train):\n",
    "        x = self.layer(x_train)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7mMfQk5KNO4"
   },
   "outputs": [],
   "source": [
    "def generate_and_train_bayesian_model( epochs=1000, plot_loss=True ):\n",
    "    \"\"\"\n",
    "    Train a Bayesian neural network, optionally plot the loss during training\n",
    "    (to check convergence), and return the corresponding neural network object\n",
    "    \"\"\"\n",
    "    model = BayesianModel()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "\n",
    "    loss_list = []\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "\n",
    "        loss = model.sample_elbo(inputs=normed_train_x,\n",
    "                                 labels=normed_train_y,\n",
    "                                 criterion=nn.MSELoss(),\n",
    "                                 sample_nbr=2,\n",
    "                                 complexity_cost_weight=1./normed_train_x.shape[0])\n",
    "        loss_list.append( float(loss) )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.plot( loss_list )\n",
    "        plt.xlabel( 'Epoch' )\n",
    "        plt.ylabel( 'Training loss' )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad53LuILKNO4",
    "outputId": "c2d380dd-0da8-4c7d-fc07-2af5d7858569",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284655118,
     "user_tz": 360,
     "elapsed": 21255,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "bayesian_model = generate_and_train_bayesian_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAay60I-KNO4"
   },
   "source": [
    "> ### **Task:**\n",
    "> Plot samples with uncertainty bands for the Bayesian neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeHJW_vNKNO4",
    "outputId": "1f078ac7-2d1d-492a-90d6-87fe82c09d66",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1738284655430,
     "user_tz": 360,
     "elapsed": 314,
     "user": {
      "displayName": "Juan Pablo Gonzalez Aguilera",
      "userId": "03694953690614948687"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aVBZRtkKNO4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "3tiqkaL7KNOw"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
