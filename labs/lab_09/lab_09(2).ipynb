{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09 - Reinforcement Learning\n",
    "\n",
    "In reinforcement learning (RL), an \"agent\" chooses actions given information about the present state of the system, and is given a reward (a measure of how good the action was). The aim is to maximize the cumulative returned reward over many interactions with the system.\n",
    "\n",
    "The problem is defined as a series of state transitions consisting of: State --> Action --> New State + reward.\n",
    "\n",
    "Multiple sequences make up an \"episode\" consisting of N state transitions.\n",
    "\n",
    "</pre>\n",
    "<img src=\"notebook_images/RL.png\" width=\"600\"> \n",
    "<pre>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll look at the Deep Deterministic Policy Gradient (DDPG) algorithm (see paper [here](https://arxiv.org/abs/1509.02971)for the standard **pendulum problem** (balancing a pendulum upright by applying torque): \n",
    "    - see problem details here https://github.com/openai/gym/wiki/Pendulum-v0\n",
    "    - see examples of performance of different agents: https://github.com/openai/gym/wiki/Leaderboard#pendulum-v0\n",
    "\n",
    "\n",
    "DDPG is an example of actor-critic RL where a mapping from observed to actions (i.e. a  \"policy\" or \"actor\") and an estimate of the long-term value of taking actions in a given state (i.e. a \"critic\") are both learned functions parameterized by neural networks. The critic provides the training signal for the actor, and the returned rewards provide the training signal for the critic.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Jump to pendulum section here ](#pendulum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the '/home/vagrant/.pyenv/versions/3.7.2/envs/py3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/uspas/optimization_and_ml --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (0.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.1 in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (from gym) (4.10.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (from gym) (1.21.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym) (3.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the '/home/vagrant/.pyenv/versions/3.7.2/envs/py3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "from os import path\n",
    "\n",
    "#import toy accelerator package\n",
    "from uspas_ml.accelerator_toy_models import simple_lattices\n",
    "from uspas_ml.utils import transformer\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at the DDPGAgent class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg.models import Critic, Actor\n",
    "from common.exp_replay import ExperienceReplayLog\n",
    "from common.noise import OUNoise\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    \n",
    "    def __init__(self, env, gamma, tau, buffer_maxlen, critic_learning_rate, actor_learning_rate, max_action = 1):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.noise = OUNoise(env.action_space)\n",
    "        self.iter = 0.0\n",
    "        self.noisy = False\n",
    "        \n",
    "        self.max_action= max_action\n",
    "        \n",
    "        #print(self.action_dim)\n",
    "        #print(self.obs_dim)\n",
    "        \n",
    "        # RL hyperparameters\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Initialize critic and actor networks\n",
    "        self.critic = Critic(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.critic_target = Critic(self.obs_dim, self.action_dim).to(self.device)\n",
    "        \n",
    "        self.actor = Actor(self.obs_dim, self.action_dim,self.max_action).to(self.device)\n",
    "        self.actor_target = Actor(self.obs_dim, self.action_dim).to(self.device)\n",
    "    \n",
    "        # Copy target network paramters for critic\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Set Optimization algorithms\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_learning_rate)\n",
    "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=actor_learning_rate)\n",
    "    \n",
    "        self.replay_buffer = ExperienceReplayLog(buffer_maxlen)        \n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        #print('obs;',obs)\n",
    "        \n",
    "        if self.noisy == True:\n",
    "            state = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "            action = self.actor.forward(state)\n",
    "            action = action.squeeze(0).cpu().detach().numpy()\n",
    "            action = self.noise.get_action(action,self.iter)\n",
    "            self.iter = self.iter+1\n",
    "        \n",
    "        else:\n",
    "            state = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "            action = self.actor.forward(state)\n",
    "            action = action.squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        \n",
    "        #Batch updates\n",
    "        states, actions, rewards, next_states, _ = self.replay_buffer.sample(batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, masks = self.replay_buffer.sample(batch_size)\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        masks = torch.FloatTensor(masks).to(self.device)\n",
    "   \n",
    "        # Q info updates\n",
    "        curr_Q = self.critic.forward(state_batch, action_batch)\n",
    "        next_actions = self.actor_target.forward(next_state_batch)\n",
    "        next_Q = self.critic_target.forward(next_state_batch, next_actions.detach())\n",
    "        expected_Q = reward_batch + self.gamma * next_Q\n",
    "        \n",
    "        # Update Critic network\n",
    "        q_loss = F.mse_loss(curr_Q, expected_Q.detach())\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        q_loss.backward() \n",
    "        \n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update Actor network\n",
    "        policy_loss = -self.critic.forward(state_batch, self.actor.forward(state_batch)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update Actor and Critic target networks \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "       \n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pendulum'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at how the Pendulum environment is defined in openAI gym below (this is the standard openAI gym implementation, and we've added additional helper functions for plotting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Description from OpenAI Gym:\n",
    "- `x-y`: cartesian coordinates of the pendulum's end in meters.\n",
    "- `theta`: angle in radians.\n",
    "- `tau`: torque in `N * m`. Defined as positive _counter-clockwise_.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The action is the torque applied to the pendulum.\n",
    "\n",
    "|Action | Min  | Max |  \n",
    "|-------|------|-----|\n",
    "|Torque | -2.0 | 2.0 |\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The observations correspond to the `x-y` coordinate of the pendulum's end, and its angular velocity.\n",
    "\n",
    "| Num | Observation      | Min  | Max |\n",
    "|-----|------------------|------|-----|\n",
    "| 0   | x = cos(theta)   | -1.0 | 1.0 |\n",
    "| 1   | y = sin(angle)   | -1.0 | 1.0 |\n",
    "| 2   | Angular Velocity | -8.0 | 8.0 |\n",
    "\n",
    "### Rewards\n",
    "\n",
    "The reward is defined as:\n",
    "\n",
    "`r = -(theta^2 + 0.1*theta_dt^2 + 0.001*torque^2)`\n",
    "\n",
    "where `theta` is the pendulum's angle normalized between `[-pi, pi]`.\n",
    "\n",
    "Based on the above equation, the minimum reward that can be obtained is `-(pi^2 + 0.1*8^2 +\n",
    "0.001*2^2) = -16.2736044`, while the maximum reward is zero (pendulum is\n",
    "upright with zero velocity and no torque being applied).\n",
    "\n",
    "### Starting State\n",
    "\n",
    "A random angle in `[-pi, pi]` and a random angular velocity in `[-1,1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PendulumEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 30\n",
    "    }\n",
    "\n",
    "    def __init__(self, g=10.0):\n",
    "        self.max_speed = 8\n",
    "        self.max_torque = 2.\n",
    "        self.dt = .05\n",
    "        self.g = g\n",
    "        self.m = 1.\n",
    "        self.l = 1.\n",
    "        self.viewer = None\n",
    "\n",
    "        #define action and observation space\n",
    "        high = np.array([1., 1., self.max_speed], dtype=np.float32)\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=-self.max_torque,\n",
    "            high=self.max_torque, shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-high,\n",
    "            high=high,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.seed()\n",
    "        \n",
    "        #lists for plotting\n",
    "        self.q1s=[]\n",
    "        self.costs=[]\n",
    "        self.reset_ep=[]\n",
    "        self.plot = True\n",
    "        \n",
    "\n",
    "    def log_results(self,q11, reward, reset_ep_flag  = False):\n",
    "        \n",
    "        '''\n",
    "        logs the results of a given iteration, for quad inputs, the reward, and whether the episode was resett\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.costs.append(reward)\n",
    "        self.q1s.append(q11)\n",
    "\n",
    "        \n",
    "        \n",
    "        if reset_ep_flag == True:\n",
    "            self.reset_ep.append(1)\n",
    "            \n",
    "        else:\n",
    "            self.reset_ep.append(0)\n",
    "\n",
    "    def plot_results(self,):\n",
    "        \n",
    "        '''\n",
    "        plots the results from the logged values\n",
    "        \n",
    "        '''\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "\n",
    "        f = plt.figure(figsize=(25,3))\n",
    "        ax = f.add_subplot(141)\n",
    "        ax2 = f.add_subplot(142)\n",
    "        \n",
    "            \n",
    "        plot_reset = np.where(np.array(self.reset_ep)==1)[0]\n",
    "        for i in range(0, len(plot_reset)):\n",
    "            ax.axvline(x=plot_reset[i],alpha=0.1,color='k')\n",
    "            ax2.axvline(x=plot_reset[i],alpha=0.1,color='k')\n",
    "                \n",
    "        ax.plot(self.q1s,'.')\n",
    "        ax.set_ylabel('action',fontsize=12)\n",
    "        ax2.plot(self.costs, 'k.')\n",
    "        ax2.set_ylabel('reward',fontsize=12)\n",
    "        \n",
    "        ax.set_xlabel('Iteration',fontsize=12)\n",
    "        ax2.set_xlabel('Iteration',fontsize=12)\n",
    "\n",
    "\n",
    "            \n",
    "        plt.show();\n",
    "        \n",
    "    def reset_plot(self,):\n",
    "        \n",
    "        '''\n",
    "        resets the logs and the plot\n",
    "        \n",
    "        '''\n",
    "        self.costs=[]\n",
    "        self.q1s=[]\n",
    "        self.reset_ep=[]\n",
    "        \n",
    "        \n",
    "    def save_plot(self,name = 'mon_'):\n",
    "        \n",
    "        '''\n",
    "        saves results from the logged values to a json file\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        run_details = {\n",
    "            'q1': self.q1s,\n",
    "            'costs': self.costs,\n",
    "            'reset_ep': self.reset_ep,\n",
    "        } \n",
    "\n",
    "        with open(name + '.json', 'w') as json_file:\n",
    "            json.dump(run_details, json_file, default = to_serializable)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, u):\n",
    "        th, thdot = self.state  # th := theta\n",
    "\n",
    "        g = self.g\n",
    "        m = self.m\n",
    "        l = self.l\n",
    "        dt = self.dt\n",
    "\n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "        self.last_u = u  # for rendering\n",
    "        costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)\n",
    "\n",
    "        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt\n",
    "        newth = th + newthdot * dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.log_results(u, -costs, reset_ep_flag = False)\n",
    "        \n",
    "\n",
    "        if self.plot == True:\n",
    "            self.plot_results()\n",
    "\n",
    "        else:\n",
    "            clear_output()\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "\n",
    "        \n",
    "        return self._get_obs(), -costs, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        high = np.array([np.pi, 1])\n",
    "        self.state = self.np_random.uniform(low=-high, high=high)\n",
    "        self.last_u = None\n",
    "        \n",
    "        self.log_results(np.nan, np.nan, reset_ep_flag = True)\n",
    "        \n",
    "    \n",
    "        if self.plot == True:\n",
    "            self.plot_results()\n",
    "\n",
    "        else:\n",
    "            clear_output()\n",
    "\n",
    "        \n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        theta, thetadot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), thetadot])\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(500, 500)\n",
    "            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)\n",
    "            rod = rendering.make_capsule(1, .2)\n",
    "            rod.set_color(.8, .3, .3)\n",
    "            self.pole_transform = rendering.Transform()\n",
    "            rod.add_attr(self.pole_transform)\n",
    "            self.viewer.add_geom(rod)\n",
    "            axle = rendering.make_circle(.05)\n",
    "            axle.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(axle)\n",
    "            fname = path.join(path.dirname(__file__), \"assets/clockwise.png\")\n",
    "            self.img = rendering.Image(fname, 1., 1.)\n",
    "            self.imgtrans = rendering.Transform()\n",
    "            self.img.add_attr(self.imgtrans)\n",
    "\n",
    "        self.viewer.add_onetime(self.img)\n",
    "        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)\n",
    "        if self.last_u:\n",
    "            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return (((x+np.pi) % (2*np.pi)) - np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the agent and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from common.utils import mini_batch_train\n",
    "from ddpg.ddpg import DDPGAgent\n",
    "\n",
    "#initialize environment\n",
    "env = PendulumEnv() # alternative: gym.make(\"Pendulum-v0\")\n",
    "\n",
    "max_episodes = 10  #total number of episodes to stop training after\n",
    "max_steps = 100 #max number of steps per episode\n",
    "batch_size = 300 #batch size for updates\n",
    "buffer_maxlen = 100000 #max buffer size\n",
    "\n",
    "# define training hyperparameters\n",
    "gamma = 0.99 #discount factor\n",
    "tau = 1e-2  #for updates with target network\n",
    "\n",
    "critic_lr = 1e-3 #learning rate\n",
    "actor_lr = 1e-3 #learning rate\n",
    "\n",
    "#initialize agent\n",
    "agent = DDPGAgent(env, gamma, tau, buffer_maxlen, critic_lr, actor_lr, max_action = 2) #max_action is set by what the environment expects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train with mini-batches and plotting for 10 episodes\n",
    "max_episodes = 5\n",
    "episode_rewards, env = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: how does the agent performance look early in training? What would you expect to see?\n",
    "Now continue trainining without plotting (plotting slows down execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faster training without plotting for 50 more episodes\n",
    "env.plot = False\n",
    "max_episodes = 50\n",
    "episode_rewards, env = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results of that previous training\n",
    "env.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: how does the agent performance look as training progresses? What would you expect to see?\n",
    "Now examine the agent's behavior at present for 5 episodes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rest plot and see how training is going with live plotting\n",
    "env.reset_plot()\n",
    "env.plot = True\n",
    "max_episodes = 5\n",
    "episode_rewards, env = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue training\n",
    "env.plot = False\n",
    "max_episodes = 90\n",
    "episode_rewards, env = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rest plot and see how training is going with live plotting\n",
    "env.reset_plot()\n",
    "env.plot = True\n",
    "max_episodes = 5\n",
    "episode_rewards, env = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: how does the agent performance look after a lot of training? \n",
    " How do the actions it takes and the reward obtained in each episode compare to early on in training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='beamsize'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
